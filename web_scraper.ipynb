{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web_scraper_yahoo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPf8IM7SfdO5nBjjY1gqqBm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kethlyncampos/webscraper/blob/main/web_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ2KkZGzSrnk",
        "outputId": "429dba89-5ace-4b2a-b59f-c8b9d64e9bc3"
      },
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install requests\n",
        "!pip install iso8601"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [829 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,813 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,459 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [723 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [930 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,444 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [689 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,896 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,225 kB]\n",
            "Fetched 14.3 MB in 4s (3,417 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "43 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 94.0 MB of archives.\n",
            "After this operation, 324 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 95.0.4638.69-0ubuntu0.18.04.1 [1,135 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 95.0.4638.69-0ubuntu0.18.04.1 [83.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 95.0.4638.69-0ubuntu0.18.04.1 [4,249 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 95.0.4638.69-0ubuntu0.18.04.1 [4,986 kB]\n",
            "Fetched 94.0 MB in 5s (20.0 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_95.0.4638.69-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_95.0.4638.69-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (95.0.4638.69-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n",
            "\u001b[K     |████████████████████████████████| 958 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 51.9 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure]~=1.26\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 60.0 MB/s \n",
            "\u001b[?25hCollecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 41.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.21)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.0 h11-0.12.0 outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.1.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.7 wsproto-1.0.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.7\n",
            "    Uninstalling urllib3-1.26.7:\n",
            "      Successfully uninstalled urllib3-1.26.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "selenium 4.1.0 requires urllib3[secure]~=1.26, but you have urllib3 1.25.11 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed urllib3-1.25.11\n",
            "Collecting iso8601\n",
            "  Downloading iso8601-1.0.2-py3-none-any.whl (9.7 kB)\n",
            "Installing collected packages: iso8601\n",
            "Successfully installed iso8601-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJBceqNAS7Sw"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "import iso8601\n",
        "from dateutil import tz\n",
        "from google.colab import drive\n",
        "from selenium.webdriver.common.by import By\n",
        "import re\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlZObRxMTZl7",
        "outputId": "658ab67d-6221-4ad5-f18f-a5dff2c0584d"
      },
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bebDQUaTnhu"
      },
      "source": [
        "yahoo = pd.read_csv('/content/gdrive/MyDrive/news_yahoo.csv')\n",
        "infomoney = pd.read_csv('/content/gdrive/MyDrive/news_infomoney.csv', lineterminator='\\n')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxuD3W8_Tsqu"
      },
      "source": [
        "comp_yahoo = []\n",
        "comp_infomoney = []\n",
        "\n",
        "for c in yahoo['URL']:\n",
        "  comp_yahoo.append(c)\n",
        "\n",
        "for c in infomoney['URL']:\n",
        "  comp_infomoney.append(c)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvLBTaMMS_VR"
      },
      "source": [
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument(\"--disable-notifications\")\n",
        "options.add_argument(\"start-maximized\")\n",
        "options.add_argument('disable-infobars')\n",
        "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36\")\n",
        "browser = webdriver.Chrome(options=options)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYq9ahFKTDAZ"
      },
      "source": [
        "def req_yahoo(url):\n",
        "  browser.get(url)\n",
        "  scrollar(browser)\n",
        "  links = browser.find_elements(By.CSS_SELECTOR, 'a.js-content-viewer.rapidnofollow')\n",
        "  noticias = []\n",
        "\n",
        "  for link in links:\n",
        "    if (('https://br.yahoo.com/news/' not in link.get_attribute('href')) and (link.get_attribute('href') not in comp_yahoo)):\n",
        "      browser.execute_script(\"arguments[0].click();\", link)\n",
        "      scrollar(browser)\n",
        "      WebDriverWait(browser, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
        "      temp = BeautifulSoup(browser.page_source, 'html.parser')\n",
        "      title = temp.find('h1', attrs={'data-test-locator':'headline'}).text\n",
        "      author = temp.find('span', attrs={'class':'caas-author-byline-collapse'}).text\n",
        "      collect_date = convert_timezone(datetime.now())\n",
        "      publication_date = convert_timezone(iso8601.parse_date(temp.find('time')['datetime']))\n",
        "      description = temp.find('div', attrs={'class':'caas-body'})\n",
        "      try:\n",
        "        description.figure.decompose()\n",
        "        description = description.text\n",
        "      except:\n",
        "        description = description.text\n",
        "      noticias.append(['Yahoo Finanças',title, description, author, publication_date, collect_date, browser.current_url])\n",
        "      browser.back()\n",
        "      sleep(3)\n",
        "  browser.close()\n",
        "  return noticias"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az8_N1QSOt52"
      },
      "source": [
        "def req_infomoney(url):\n",
        "  browser.get(url)\n",
        "  browser.maximize_window()\n",
        "  alert = browser.switch_to.active_element.click\n",
        "  infinite = browser.find_element(By.ID, 'infinite-handle')\n",
        "  browser.execute_script(\"arguments[0].click();\", infinite)\n",
        "  count = 40\n",
        "  while(count > 0):\n",
        "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    WebDriverWait(browser, 40).until(EC.presence_of_element_located((By.ID, \"infinite-handle\")))\n",
        "    browser.execute_script(\"arguments[0].click();\", infinite)\n",
        "    newHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
        "    lastHeight = newHeight\n",
        "    count-=1\n",
        "  page = BeautifulSoup(browser.page_source, 'html.parser')\n",
        "  browser.close()\n",
        "  links = page.find_all(class_=\"hl-title\")\n",
        "  lista_url = []\n",
        "  for j in links:\n",
        "    aux = j.find('a')\n",
        "    if(aux):\n",
        "      if \"stock-pickers\" not in aux['href']:\n",
        "        lista_url.append(aux['href'])\n",
        "  lista_url = set(lista_url)\n",
        "  noticias = []\n",
        "  subtitle = \"\"\n",
        "  for k in lista_url:\n",
        "    if k not in comp_infomoney:\n",
        "      response = requests.get(k)\n",
        "      temp = BeautifulSoup(response.content, 'html.parser')\n",
        "      title = temp.find('h1', attrs={'class': 'page-title-1'})\n",
        "      if title: # se não houver título, a página é do tipo cotações e possui apenas as informações sobre a empresa\n",
        "        title = title.text\n",
        "        subtitle = trata_string(temp.find('p', attrs={'class': 'article-lead'}).text)\n",
        "        author = temp.find('span', attrs={'class':'author-name'})\n",
        "        if author:\n",
        "          author = trata_string(author.find('a').text) # se não houver autor, a página é do tipo guias\n",
        "          collect_date = convert_timezone(datetime.now())\n",
        "          publication_date = iso8601.parse_date(temp.find('time', attrs={'class':'entry-date'})['datetime']).strftime('%Y-%m-%d %H:%M:%S')\n",
        "          description = temp.find('div', attrs={'class':'article-content'})\n",
        "          try:\n",
        "            description.figure.decompose()\n",
        "            description = description.text\n",
        "          except:\n",
        "            description = description.text\n",
        "          description = trata_string(description)\n",
        "          noticias.append(['Infomoney',title, subtitle, description, author, publication_date, collect_date, k])\n",
        "  return noticias"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhCKn0Hny0m5"
      },
      "source": [
        "def req_investing(url):\n",
        "  browser2 = webdriver.Chrome(options=options)\n",
        "  browser2.get(url)\n",
        "  scrollar(browser2)\n",
        "  noticias = []\n",
        "  cont = 0\n",
        "  while(True):\n",
        "    links = browser2.find_elements(By.CSS_SELECTOR, '.js-article-item.articleItem .textDiv a')\n",
        "    for link in links:\n",
        "      WebDriverWait(browser2, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, '.js-article-item.articleItem .textDiv a')))\n",
        "      browser2.execute_script(\"arguments[0].click();\", link)\n",
        "      scrollar(browser2)\n",
        "      temp = BeautifulSoup(browser2.page_source, 'html.parser')\n",
        "      title = temp.find('h1', attrs={'class':'articleHeader'}).text\n",
        "      time = temp.find('div', attrs={'class':'contentSectionDetails'})\n",
        "      time = time.find('span').text\n",
        "      time = re.search('\\(([^)]+)', time).group(1)\n",
        "      publication_date = datetime.strptime(time, '%d.%m.%Y %H:%M')\n",
        "      publication_date = datetime.strftime(publication_date, '%Y-%m-%d %H:%M:%S')\n",
        "      collect_date = convert_timezone(datetime.now())\n",
        "      description = temp.find('div', attrs={'class':'articlePage'})\n",
        "      try:\n",
        "        description.find('div',class_='imgCarousel').decompose()\n",
        "        description = description.text\n",
        "      except:\n",
        "        description = description.text\n",
        "      description = trata_string(description)\n",
        "      noticias.append(['Investing.com',title, description, publication_date, collect_date, browser2.current_url])\n",
        "      browser2.back()\n",
        "      sleep(2)\n",
        "      scrollar(browser2)\n",
        "    \n",
        "    proximo = browser2.find_element(By.CSS_SELECTOR, '.sideDiv.inlineblock.text_align_lang_base_2')\n",
        "    browser2.execute_script(\"arguments[0].click();\", proximo)\n",
        "    scrollar(browser2)\n",
        "    print(browser2.current_url)\n",
        "    break\n",
        "  browser2.close()\n",
        "  return noticias"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpi-WFnFTGi6"
      },
      "source": [
        "def scrollar(driver):\n",
        "  lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "  while True:\n",
        "      driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "      sleep(5)\n",
        "      newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "      if newHeight == lastHeight:\n",
        "          break\n",
        "      lastHeight = newHeight\n",
        "  return"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86_FH_jcTLSV"
      },
      "source": [
        "def convert_timezone(date_time):\n",
        "  sp = tz.gettz('America/Sao_Paulo')\n",
        "  new_date = date_time.replace(tzinfo=pytz.utc)\n",
        "  new_date = new_date.astimezone(sp)\n",
        "  new_date = new_date.strftime('%Y-%m-%d %H:%M:%S')\n",
        "  return new_date"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYtWcU_JTNv8"
      },
      "source": [
        "def trata_string(data):\n",
        "  data = re.sub(r'[\\t\\n ]+', ' ', data)\n",
        "  return data"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHdfAK0iTPBH"
      },
      "source": [
        "array_noticias = req_yahoo('https://br.yahoo.com/topics/bolsa-de-valores/')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-OQ18uMTfCZ"
      },
      "source": [
        "dataset_temp = pd.DataFrame(array_noticias,columns=['Fonte', 'Título', 'Descrição', 'Autor', 'Data/Hora de Publicação', 'Data/Hora de Coleta', 'URL'])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H4HcTrIx3Bd"
      },
      "source": [
        "if len(dataset_temp) > 0:\n",
        "  dataset_temp.drop_duplicates(subset=['Descrição'], inplace = True)\n",
        "  dataset_temp.reset_index(drop = True, inplace = True)\n",
        "  yahoo = pd.concat([dataset_temp,yahoo],ignore_index=True)\n",
        "  yahoo.drop_duplicates(subset=['Descrição'], inplace = True)\n",
        "  yahoo.drop(yahoo.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
        "  yahoo = yahoo.reset_index(drop=True)\n",
        "  yahoo.to_csv('/content/gdrive/MyDrive/news_yahoo.csv', index=False)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H1GyVnMlpc3"
      },
      "source": [
        "noticias_info = req_infomoney(\"https://www.infomoney.com.br/mercados/\")"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6wxon6ZUb_w"
      },
      "source": [
        "dataset_temp = pd.DataFrame(noticias_info,columns=['Fonte', 'Título', 'Subtítulo','Descrição', 'Autor', 'Data/Hora de Publicação', 'Data/Hora de Coleta', 'URL'])"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V93b35qCUdqP"
      },
      "source": [
        "if len(dataset_temp) > 0:\n",
        "  dataset_temp.drop_duplicates(subset=['Descrição'], inplace = True)\n",
        "  dataset_temp.reset_index(drop = True, inplace = True)\n",
        "  infomoney = pd.concat([dataset_temp, infomoney])\n",
        "  infomoney.drop_duplicates(subset=['Descrição'], inplace = True)\n",
        "  infomoney.drop(infomoney.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
        "  infomoney = infomoney.reset_index(drop=True)\n",
        "  infomoney.to_csv('/content/gdrive/MyDrive/news_infomoney.csv', index=False)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpaaJsGtyymp"
      },
      "source": [
        "page2 = req_investing('https://br.investing.com/news/stock-market-news')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqzcCOfAeef0"
      },
      "source": [
        "dataset_temp = pd.DataFrame(array_noticias,columns=['Fonte', 'Título', 'Descrição', 'Data/Hora de Publicação', 'Data/Hora de Coleta', 'URL'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}